{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daae7af2",
   "metadata": {},
   "source": [
    "#  导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490664e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7365821",
   "metadata": {},
   "source": [
    "# 定义要处理的模型(YOLOv8n目标检测模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO(model='./models/yolov8-traffic.pt')\n",
    "yolo = yolo.cuda()\n",
    "results = yolo(source=\"./images/origin_img_1.jpg\")\n",
    "for result in results:\n",
    "    print(result.boxes.cls[0])\n",
    "    result.show()\n",
    "results = yolo(source=\"./images/perturbed_img_1.jpg\")\n",
    "for result in results:\n",
    "    print(result.boxes.cls[0])\n",
    "    result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186abcb4",
   "metadata": {},
   "source": [
    "选择要处理的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87269529",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.info(detailed=True)\n",
    "yolo_model = yolo.model\n",
    "device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "yolo_model = yolo_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2685ade",
   "metadata": {},
   "source": [
    "选择要进行CAM操作的目标层,暂时只检查backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_nn = yolo_model.model\n",
    "layer_map = {\n",
    "    # 如果要选择其他c2f输出,只需要更改这个字典\n",
    "    \"backbone_c2f1\": 2,\n",
    "    \"backbone_c2f2\": 4,\n",
    "    \"backbone_c2f3\": 6,\n",
    "    \"backbone_c2f4\": 8, \n",
    "    \"backbone_sppf\": 9,\n",
    "    \"neck_c2f1\": 12,\n",
    "    \"neck_c2f2\": 15,\n",
    "    \"neck_c2f3\": 18,\n",
    "    \"neck_c2f4\": 21\n",
    "}\n",
    "\n",
    "layers = {layer: yolo_nn[idx] for layer, idx in layer_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d3877",
   "metadata": {},
   "source": [
    "You may get an innaccurate output at first (If it's running on GPU)\n",
    "\n",
    "可视化相关配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [3.0, 3.0]\n",
    "\n",
    "image_path = \"images/perturbed_img_2.jpg\"\n",
    "img = cv2.imread(filename=image_path, flags=1)\n",
    "rgb_img = img.copy()[:, :, ::-1]\n",
    "rgb_img = cv2.resize(src=rgb_img, dsize=(224, 224))\n",
    "rgb_img = np.float32(rgb_img) / 255\n",
    "input_tensor = preprocess_image(img=rgb_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(dtype=torch.float32, device=device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5766010-9131-4edc-9555-f1b7fc3f18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_result(\n",
    "    img, model, input_tensor, chosen_layers, virtualize=True)-> np.ndarray:\n",
    "    # all_layers is a dict, but target_layers must be list\n",
    "    target_layers = [layers[layer_name] for layer_name in chosen_layers]\n",
    "    with EigenCAM(model=model, target_layers=target_layers) as cam:\n",
    "        # cam.batch_size = 1\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=target_layers) \n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        cam_image = show_cam_on_image(img=img, mask=grayscale_cam, use_rgb=True)  # type: ignore\n",
    "        if virtualize:\n",
    "            plt.imshow(X=cam_image)\n",
    "            plt.title(label=f\"EigenCAM in {chosen_layers}\")\n",
    "            plt.show()\n",
    "        return grayscale_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c3e81",
   "metadata": {},
   "source": [
    "各c2f层CAM结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_backbone_c2f1 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ece51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_backbone_c2f2 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca04ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_backbone_c2f3 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6699159",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_backbone_c2f4 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_backbone_sppf = get_cam_result(\n",
    "    img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_sppf\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_neck_c2f1 = get_cam_result(\n",
    "    img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_neck_c2f2 = get_cam_result(\n",
    "    img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a93e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_neck_c2f3 = get_cam_result(\n",
    "    img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_neck_c2f4 = get_cam_result(\n",
    "    img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2957407",
   "metadata": {},
   "source": [
    "**评估指标:**\n",
    "\n",
    "**ROAD(Region Of Attention Discirmination):** (有问题:`ClassifierOutputTarget(category=17)`这一部分意义不明)\n",
    "\n",
    "- `ROADMostRelevantFirstAverage`: 逐步移除置信度从高到地的CAM区域,比较移除前后模型预测置信度的区别,相差越大,代表高相关区域定位准确.\n",
    "- `ROADLeastRelevantFirstAverage`: 逐步移除置信度从低到高的CAM区域,比较移除前后模型预测置信度的区别,相差越小,代表无关区域没有被错误定位到."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.metrics.road import ROADMostRelevantFirstAverage\n",
    "from pytorch_grad_cam.metrics.road import ROADLeastRelevantFirstAverage\n",
    "\n",
    "def eval_ROADMostRelevantFirst(model, input_tensor, grayscale_cams, target_classes) -> None:\n",
    "    cam_metric = ROADMostRelevantFirstAverage(percentiles=[20, 40, 60, 80])\n",
    "    # all_layers is a dict, but target_layers must be list\n",
    "\n",
    "    scores = cam_metric(\n",
    "        model=model, input_tensor=input_tensor, cams=grayscale_cams, targets=target_classes,\n",
    "    )\n",
    "    # print(scores.size)\n",
    "    score = scores.sum()\n",
    "    print(f\"The average confidence increase with ROAD accross 4 thresholds: {score}\")\n",
    "\n",
    "\n",
    "# You can also average across different percentiles, and combine\n",
    "# (LeastRelevantFirst - MostRelevantFirst) / 2\n",
    "\n",
    "batch_cam_backbone_c2f1 = torch.from_numpy(cam_backbone_c2f1).unsqueeze(dim=0)\n",
    "\n",
    "eval_ROADMostRelevantFirst(\n",
    "    model=yolo.model, input_tensor=input_tensor, grayscale_cams=batch_cam_backbone_c2f1.numpy(), target_classes=[ClassifierOutputTarget(category=17)]\n",
    ")\n",
    "\n",
    "def eval_ROADLeastRelevantFirstAverage(model, input_tensor, grayscale_cams, target_classes) -> None:\n",
    "    cam_metric = ROADLeastRelevantFirstAverage(percentiles=[20, 40, 60, 80])\n",
    "    # all_layers is a dict, but target_layers must be list\n",
    "\n",
    "    scores = cam_metric(\n",
    "        model=model,\n",
    "        input_tensor=input_tensor,\n",
    "        cams=grayscale_cams,\n",
    "        targets=target_classes,\n",
    "    )\n",
    "    # print(scores.size)\n",
    "    score = scores.sum()\n",
    "    print(f\"The average confidence increase with ROAD accross 4 thresholds: {score}\")\n",
    "\n",
    "\n",
    "# You can also average across different percentiles, and combine\n",
    "# (LeastRelevantFirst - MostRelevantFirst) / 2\n",
    "\n",
    "eval_ROADLeastRelevantFirstAverage(\n",
    "    model=yolo.model,\n",
    "    input_tensor=input_tensor,\n",
    "    grayscale_cams=batch_cam_backbone_c2f1.numpy(),\n",
    "    target_classes=[ClassifierOutputTarget(category=17)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875af718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(8, 8))\n",
    "\n",
    "axes[0].imshow(cam_backbone_c2f1, cmap=\"gray\")\n",
    "axes[0].set_title(\"cam_b_c2f1\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(cam_backbone_c2f2, cmap=\"gray\")\n",
    "axes[1].set_title(\"cam_b_c2f2\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(cam_backbone_c2f3, cmap=\"gray\")\n",
    "axes[2].set_title(\"cam_b_c2f3\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[3].imshow(cam_backbone_c2f4, cmap=\"gray\")\n",
    "axes[3].set_title(\"cam_b_c2f4\")\n",
    "axes[3].axis(\"off\")\n",
    "axes[4].imshow(cam_backbone_sppf, cmap=\"gray\")\n",
    "axes[4].set_title(\"cam_backbone_sppf\")\n",
    "axes[4].axis(\"off\")\n",
    "axes[5].imshow(cam_neck_c2f1, cmap=\"gray\")\n",
    "axes[5].set_title(\"cam_neck_c2f1\")\n",
    "axes[5].axis(\"off\")\n",
    "axes[6].imshow(cam_neck_c2f2, cmap=\"gray\")\n",
    "axes[6].set_title(\"cam_neck_c2f2\")\n",
    "axes[6].axis(\"off\")\n",
    "axes[7].imshow(cam_neck_c2f3, cmap=\"gray\")\n",
    "axes[7].set_title(\"cam_neck_c2f3\")\n",
    "axes[7].axis(\"off\")\n",
    "axes[8].imshow(cam_neck_c2f4, cmap=\"gray\")\n",
    "axes[8].set_title(\"cam_neck_c2f4\")\n",
    "axes[8].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d35086",
   "metadata": {},
   "source": [
    "**SSIM(Structural Similarity，结构相似性)**\n",
    "\n",
    "通过比较原始样本和对抗样本的热力图, 衡量注意力区域是否被显著改变, SSIM 越低 → 热力图结构差异越大（攻击更有效）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35abf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity  \n",
    "\n",
    "# im1, im2分别表示参与计算的图像数据\n",
    "# data_range表示图像数据的范围，一般设置为255或者1(如果对图像数据做了归一化操作，则为1)\n",
    "# channel_axis表示颜色通道位于图像的第几维度，如果不指定的话，则默认输入灰度图像\n",
    "ssim = structural_similarity(\n",
    "    cam_backbone_c2f1,\n",
    "    cam_backbone_c2f4,\n",
    "    win_size=None,\n",
    "    gradient=False,\n",
    "    data_range=1,\n",
    "    channel_axis=None,\n",
    "    multichannel=False,\n",
    "    gaussian_weights=False,\n",
    "    full=False,\n",
    ")\n",
    "\n",
    "print(f\"SSIM of cam_backbone_c2f1 & cam_backbone_c2f2: {ssim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a01b6",
   "metadata": {},
   "source": [
    "**在masked_pgd对抗攻击上测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686045c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_traffic = YOLO(model='./models/yolov8-traffic.pt')\n",
    "yolo_traffic = yolo_traffic.cuda()\n",
    "yolo_traffic_model = yolo_traffic.model.to(device)\n",
    "yolo_traffic_nn = yolo_traffic_model.model\n",
    "layers = {layer: yolo_traffic_nn[idx] for layer, idx in layer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_img = cv2.imread(filename=\"./images/origin_img_2.jpg\", flags=1)\n",
    "rgb_origin_img = origin_img.copy()[:, :, ::-1]\n",
    "rgb_origin_img = cv2.resize(src=rgb_origin_img, dsize=(224, 224))\n",
    "rgb_origin_img = np.float32(rgb_origin_img) / 255\n",
    "input_tensor_origin_img = preprocess_image(img=rgb_origin_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(dtype=torch.float32, device=device)  # type: ignore\n",
    "perturbed_img = cv2.imread(filename=\"./images/perturbed_img_2.jpg\", flags=1)\n",
    "rgb_perturbed_img = perturbed_img.copy()[:, :, ::-1]\n",
    "rgb_perturbed_img = cv2.resize(src=rgb_perturbed_img, dsize=(224, 224))\n",
    "rgb_perturbed_img = np.float32(rgb_perturbed_img) / 255\n",
    "input_tensor_perturbed_img = preprocess_image(img=rgb_perturbed_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(dtype=torch.float32, device=device)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "cams_origin_img = {\n",
    "    layer: get_cam_result(\n",
    "        img=rgb_origin_img,\n",
    "        model=yolo_traffic_model,\n",
    "        input_tensor=input_tensor_origin_img,\n",
    "        chosen_layers=[layer],\n",
    "        virtualize=False,\n",
    "    )\n",
    "    for layer in layer_map.keys()\n",
    "}\n",
    "\n",
    "cams_perturbed_img = {\n",
    "    layer: get_cam_result(\n",
    "        img=rgb_perturbed_img,\n",
    "        model=yolo_traffic_model,\n",
    "        input_tensor=input_tensor_perturbed_img,\n",
    "        chosen_layers=[layer],\n",
    "        virtualize=False,\n",
    "    )\n",
    "    for layer in layer_map.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1302eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SSIM of cam_origin_img and cam_perturbed_img: \")\n",
    "for chosen_layer in layer_map.keys():\n",
    "    # chosen_layer = \"backbone_c2f4\"  # 在这里选择要比较的层次\n",
    "    cam_origin_img = cams_origin_img[chosen_layer]\n",
    "    cam_perturbed_img = cams_perturbed_img[chosen_layer]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4, 4))\n",
    "    axes[0].imshow(cam_origin_img, cmap=\"gray\")\n",
    "    axes[0].set_title(f\"orig_{chosen_layer}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(cam_perturbed_img, cmap=\"gray\")\n",
    "    axes[1].set_title(f\"attk_{chosen_layer}\")\n",
    "    axes[1].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    ssim = structural_similarity(cam_origin_img, cam_perturbed_img, data_range=1)\n",
    "    print(f\"{chosen_layer}: {ssim}\")\n",
    "    \n",
    "# layer_map = {\n",
    "#     # 如果要选择其他c2f输出,只需要更改这个字典\n",
    "#     \"backbone_c2f1\": 2,\n",
    "#     \"backbone_c2f2\": 4,\n",
    "#     \"backbone_c2f3\": 6,\n",
    "#     \"backbone_c2f4\": 8, \n",
    "#     \"backbone_sppf\": 9,\n",
    "#     \"neck_c2f1\": 12,\n",
    "#     \"neck_c2f2\": 15,\n",
    "#     \"neck_c2f3\": 18,\n",
    "#     \"neck_c2f4\": 21\n",
    "# }\n",
    "\n",
    "# S\n",
    "# SSIM of cam_origin_img and cam_perturbed_img: \n",
    "# backbone_c2f1: 0.9997942613480257\n",
    "# backbone_c2f2: 0.9988800610291236\n",
    "# backbone_c2f3: 0.9993543305044151\n",
    "# backbone_c2f4: 0.9972734578256705\n",
    "# backbone_sppf: 0.9987707076693472\n",
    "# neck_c2f1: 0.9793938244919129\n",
    "# neck_c2f2: 0.9689466165305084\n",
    "# neck_c2f3: 0.9822124863635567\n",
    "# neck_c2f4: 0.986672852255135\n",
    "\n",
    "# L\n",
    "# SSIM of cam_origin_img and cam_perturbed_img: \n",
    "# backbone_c2f1: 0.9636343326863902\n",
    "# backbone_c2f2: 0.9781642958550689\n",
    "# backbone_c2f3: 0.9551331287842689\n",
    "# backbone_c2f4: 0.9047849218385144\n",
    "# backbone_sppf: 0.036879426969998984\n",
    "# neck_c2f1: 0.8844158753992417\n",
    "# neck_c2f2: 0.8037270538466107\n",
    "# neck_c2f3: 0.5478848147437752\n",
    "# neck_c2f4: 0.957862519667803"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748485f7",
   "metadata": {},
   "source": [
    "选择中等大小目标的任务上的neck_c2f3卷积层进行逐层细化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fdf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载YOLO模型（以v8为例）\n",
    "neck_c2f3_nn = yolo_traffic_nn[layer_map['neck_c2f3']]\n",
    "# print(neck_c2f3_nn)\n",
    "# 存储特征图的字典\n",
    "features = {}\n",
    "\n",
    "# 定义钩子函数：保存当前层输出\n",
    "def hook_fn(module, input, output) -> None:\n",
    "    features[module.__repr__()] = output  # 或自定义唯一标识\n",
    "\n",
    "# 遍历c2f中的所有子模块\n",
    "def register_hooks(module) -> None:\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Conv2d):  # 筛选卷积层\n",
    "            print(child.type)\n",
    "            child.register_forward_hook(hook=hook_fn)\n",
    "        else:\n",
    "            register_hooks(module=child)\n",
    "        \n",
    "register_hooks(module=neck_c2f3_nn)\n",
    "        \n",
    "# 输入测试数据触发前向传播\n",
    "output = yolo(source='./images/perturbed_img_2.jpg')\n",
    "# 提取特征图（features字典中保存了各卷积层的输出）\n",
    "print(features.keys())  # 查看捕获的卷积层特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
