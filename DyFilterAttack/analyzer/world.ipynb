{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490664e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6b3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldDetect(\n",
       "  (cv2): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (cv3): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dfl): DFL(\n",
       "    (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (cv4): ModuleList(\n",
       "    (0-2): 3 x ContrastiveHead()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo = YOLOWorld(model='../models/yolov8s-world.pt')\n",
    "yolo = yolo.to(torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "key_layer_idx = {\n",
    "    # module 2-9 same as yolov8 \n",
    "    \"backbone_c2f1\": 2,\n",
    "    \"backbone_c2f2\": 4,\n",
    "    \"backbone_c2f3\": 6,\n",
    "    \"backbone_c2f4\": 8, \n",
    "    \"backbone_sppf\": 9,\n",
    "    # module changed\n",
    "    \"neck_c2f1\": 15,\n",
    "    \"neck_c2f2\": 19,\n",
    "    \"neck_c2f3\": 22,\n",
    "    \"detect_head\": 23\n",
    "}\n",
    "layers = {layer: yolo.model.model[idx] for layer, idx in key_layer_idx.items()}\n",
    "detect_head = layers['detect_head']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b0ea5",
   "metadata": {},
   "source": [
    "提取输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering Hook: backbone_c2f1.cv1\n",
      "Registering Hook: backbone_c2f1.cv1.conv\n",
      "Registering Hook: backbone_c2f1.cv1.bn\n",
      "Registering Hook: backbone_c2f1.cv1.act\n",
      "Registering Hook: backbone_c2f1.cv2\n",
      "Registering Hook: backbone_c2f1.cv2.conv\n",
      "Registering Hook: backbone_c2f1.cv2.bn\n",
      "Registering Hook: backbone_c2f1.cv2.act\n",
      "Registering Hook: backbone_c2f1.m\n",
      "Registering Hook: backbone_c2f1.m.0\n",
      "Registering Hook: backbone_c2f1.m.0.cv1\n",
      "Registering Hook: backbone_c2f1.m.0.cv1.conv\n",
      "Registering Hook: backbone_c2f1.m.0.cv1.bn\n",
      "Registering Hook: backbone_c2f1.m.0.cv1.act\n",
      "Registering Hook: backbone_c2f1.m.0.cv2\n",
      "Registering Hook: backbone_c2f1.m.0.cv2.conv\n",
      "Registering Hook: backbone_c2f1.m.0.cv2.bn\n",
      "Registering Hook: backbone_c2f1.m.0.cv2.act\n",
      "\n",
      "image 1/1 e:\\bmx\\DyFilterAttack\\DyFilterAttack\\analyzer\\..\\testset\\bus.jpg: 640x480 5 persons, 1 bus, 1 stop sign, 20.7ms\n",
      "Speed: 2.0ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from DyFilterAttack.analyzer.utils import SaveFeatures\n",
    "save_feats = SaveFeatures()\n",
    "save_feats.register_hooks(module=backbone_c2f1, parent_path='backbone_c2f1', verbose=True)\n",
    "img_path = '../testset/bus.jpg'\n",
    "yolo.predict(img_path)\n",
    "detect_head_raw_feats = save_feats.get_features()\n",
    "# print(detect_head_raw_feats['detect_head.cv4.1'].size())\n",
    "\n",
    "nl = detect_head.nl\n",
    "nc, reg_max =  detect_head.nc, detect_head.reg_max\n",
    "no = nc + 4 * reg_max\n",
    "assert no == detect_head.no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc489239",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'detect_head.cv2.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# process1 \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# text -> (B, nc, embed_dim)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# image -> (B, embed_dim, H, W)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# cat_result -> (B, nc + reg_max * 4, H ,W) -> (B, no, H ,W)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# x[i] -> cat_result[i] (i = 1, 2, nl)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m cv2_raw_feats = [detect_head_raw_feats[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdetect_head.cv2.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nl)]\n\u001b[32m     10\u001b[39m cv4_raw_feats = [detect_head_raw_feats[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdetect_head.cv4.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nl)]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcv2_raw_feats \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv2_raw_feats[\u001b[32m0\u001b[39m].size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# process1 \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# text -> (B, nc, embed_dim)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# image -> (B, embed_dim, H, W)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# cat_result -> (B, nc + reg_max * 4, H ,W) -> (B, no, H ,W)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# x[i] -> cat_result[i] (i = 1, 2, nl)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m cv2_raw_feats = [detect_head_raw_feats[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdetect_head.cv2.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nl)]\n\u001b[32m     10\u001b[39m cv4_raw_feats = [detect_head_raw_feats[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdetect_head.cv4.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nl)]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcv2_raw_feats \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv2_raw_feats[\u001b[32m0\u001b[39m].size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'detect_head.cv2.0'"
     ]
    }
   ],
   "source": [
    "# process1 \n",
    "# text -> (B, nc, embed_dim)\n",
    "# image -> (B, embed_dim, H, W)\n",
    "# cv4 contrast(iamge, text) -> (B, nc, H, W)\n",
    "# cv2(image) -> (B, reg_max * 4, H, W)\n",
    "# cat_result -> (B, nc + reg_max * 4, H ,W) -> (B, no, H ,W)\n",
    "# x[i] -> cat_result[i] (i = 1, 2, nl)\n",
    "\n",
    "cv2_raw_feats = [detect_head_raw_feats[f'detect_head.cv2.{i}'] for i in range(nl)]\n",
    "cv4_raw_feats = [detect_head_raw_feats[f'detect_head.cv4.{i}'] for i in range(nl)]\n",
    "\n",
    "print(f'cv2_raw_feats {0}: {cv2_raw_feats[0].size()}')\n",
    "print(f'cv2_raw_feats {1}: {cv2_raw_feats[1].size()}')\n",
    "print(f'cv2_raw_feats {2}: {cv2_raw_feats[2].size()}')\n",
    "print(f'cv4_raw_feats {0}: {cv4_raw_feats[0].size()}')\n",
    "print(f'cv4_raw_feats {1}: {cv4_raw_feats[1].size()}')\n",
    "print(f'cv4_raw_feats {2}: {cv4_raw_feats[2].size()}')\n",
    "\n",
    "# process2 (_inference)\n",
    "# flat(x[i]) -> (B, no, H * W)\n",
    "# cat(x) -> (B, C, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "# split(x) -> bbox(B, 4 * reg_max, H0 * W0 + H1 * W1 + H2 * W2), cls(logit)(B, nc, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "# docode(bbox) -> dbox\n",
    "# logit(cls) -> sigmoid(cls)\n",
    "# y -> cat(dbox, cls)\n",
    "\n",
    "cat_raw_feats = [torch.cat((cv2_raw_feats[i], cv4_raw_feats[i]), 1) for i in range(nl)]\n",
    "flatten_raw_feats = torch.cat([cat_raw_feat.view(cat_raw_feats[0].shape[0], no, -1) for cat_raw_feat in cat_raw_feats], 2)\n",
    "raw_box = flatten_raw_feats[:, : reg_max * 4]\n",
    "raw_cls = flatten_raw_feats[:, reg_max * 4 :]\n",
    "\n",
    "dfl_feats = detect_head_raw_feats['detect_head.dfl']\n",
    "dbox = detect_head.decode_bboxes(dfl_feats, detect_head.anchors.unsqueeze(0)) * detect_head.strides\n",
    "# ! Attention: we need cls(logit) as y_det\n",
    "logit_cls = raw_cls\n",
    "sigmoid_cls = logit_cls.sigmoid()\n",
    "\n",
    "print(f'cat_raw_feat: {cat_raw_feats[0].size()}')           # (B, C, H * W)\n",
    "print(f'flatten_raw_feats: {flatten_raw_feats.size()}')     # (B, C, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "print(f'raw_box: {raw_box.size()}')                         # (B, 4 * reg_max, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "print(f'raw_cls: {raw_cls.size()}')                         # (B, nc, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "print(f'dbox: {dbox.size()}')                               # (B, 4,  H0 * W0 + H1 * W1 + H2 * W2)\n",
    "print(f'logit_cls: {logit_cls.size()}')                     # (B, nc, H0 * W0 + H1 * W1 + H2 * W2)\n",
    "print(f'sigmoid_cls: {sigmoid_cls.size()}')                 # (B, nc, H0 * W0 + H1 * W1 + H2 * W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29317c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task=detect\n",
      "mode=predict\n",
      "model=../models/yolov8s-world.pt\n",
      "data=None\n",
      "epochs=100\n",
      "time=None\n",
      "patience=100\n",
      "batch=1\n",
      "imgsz=640\n",
      "save=False\n",
      "save_period=-1\n",
      "cache=False\n",
      "device=cuda:0\n",
      "workers=8\n",
      "project=None\n",
      "name=None\n",
      "exist_ok=False\n",
      "pretrained=True\n",
      "optimizer=auto\n",
      "verbose=True\n",
      "seed=0\n",
      "deterministic=True\n",
      "single_cls=False\n",
      "rect=True\n",
      "cos_lr=False\n",
      "close_mosaic=10\n",
      "resume=False\n",
      "amp=True\n",
      "fraction=1.0\n",
      "profile=False\n",
      "freeze=None\n",
      "multi_scale=False\n",
      "overlap_mask=True\n",
      "mask_ratio=4\n",
      "dropout=0.0\n",
      "val=True\n",
      "split=val\n",
      "save_json=False\n",
      "conf=0.25\n",
      "iou=0.7\n",
      "max_det=300\n",
      "half=False\n",
      "dnn=False\n",
      "plots=True\n",
      "source=None\n",
      "vid_stride=1\n",
      "stream_buffer=False\n",
      "visualize=False\n",
      "augment=False\n",
      "agnostic_nms=False\n",
      "classes=None\n",
      "retina_masks=False\n",
      "embed=None\n",
      "show=False\n",
      "save_frames=False\n",
      "save_txt=False\n",
      "save_conf=False\n",
      "save_crop=False\n",
      "show_labels=True\n",
      "show_conf=True\n",
      "show_boxes=True\n",
      "line_width=None\n",
      "format=torchscript\n",
      "keras=False\n",
      "optimize=False\n",
      "int8=False\n",
      "dynamic=False\n",
      "simplify=True\n",
      "opset=None\n",
      "workspace=None\n",
      "nms=False\n",
      "lr0=0.01\n",
      "lrf=0.01\n",
      "momentum=0.937\n",
      "weight_decay=0.0005\n",
      "warmup_epochs=3.0\n",
      "warmup_momentum=0.8\n",
      "warmup_bias_lr=0.1\n",
      "box=7.5\n",
      "cls=0.5\n",
      "dfl=1.5\n",
      "pose=12.0\n",
      "kobj=1.0\n",
      "nbs=64\n",
      "hsv_h=0.015\n",
      "hsv_s=0.7\n",
      "hsv_v=0.4\n",
      "degrees=0.0\n",
      "translate=0.1\n",
      "scale=0.5\n",
      "shear=0.0\n",
      "perspective=0.0\n",
      "flipud=0.0\n",
      "fliplr=0.5\n",
      "bgr=0.0\n",
      "mosaic=1.0\n",
      "mixup=0.0\n",
      "cutmix=0.0\n",
      "copy_paste=0.0\n",
      "copy_paste_mode=flip\n",
      "auto_augment=randaugment\n",
      "erasing=0.4\n",
      "cfg=None\n",
      "tracker=botsort.yaml\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(yolo.predictor.args)\n\u001b[32m      4\u001b[39m preds = ops.non_max_suppression(\n\u001b[32m      5\u001b[39m     flatten_raw_feats,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.conf,\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.iou,\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.classes,\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.agnostic_nms,\n\u001b[32m     10\u001b[39m     max_det=\u001b[38;5;28mself\u001b[39m.args.max_det,\n\u001b[32m     11\u001b[39m     nc=\u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.names),\n\u001b[32m     12\u001b[39m     end2end=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mend2end\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     13\u001b[39m     rotated=\u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     return_idxs=save_feats,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[32m     18\u001b[39m     orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "from ultralytics.utils import ops\n",
    "print(yolo.predictor.args)\n",
    "\n",
    "preds = ops.non_max_suppression(\n",
    "    flatten_raw_feats,\n",
    "    self.args.conf,\n",
    "    self.args.iou,\n",
    "    self.args.classes,\n",
    "    self.args.agnostic_nms,\n",
    "    max_det=self.args.max_det,\n",
    "    nc=0 if self.args.task == \"detect\" else len(self.model.names),\n",
    "    end2end=getattr(self.model, \"end2end\", False),\n",
    "    rotated=self.args.task == \"obb\",\n",
    "    return_idxs=save_feats,\n",
    ")\n",
    "\n",
    "if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n",
    "    orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
    "\n",
    "if save_feats:\n",
    "    obj_feats = self.get_obj_feats(self._feats, preds[1])\n",
    "    preds = preds[0]\n",
    "\n",
    "results = self.construct_results(preds, img, orig_imgs, **kwargs)\n",
    "\n",
    "if save_feats:\n",
    "    for r, f in zip(results, obj_feats):\n",
    "        r.feats = f  # add object features to results\n",
    "\n",
    "return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c6eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
