{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490664e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'YOLO' from 'ultralytics' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'YOLO' from 'ultralytics' (unknown location)"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO(model='../models/yolov8-gtsrb.pt')\n",
    "yolo_model = yolo.model\n",
    "yolo_nn = yolo.model.model\n",
    "yolo_model = yolo_model.to(torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "layer_map = {\n",
    "    # 如果要选择其他c2f输出,只需要更改这个字典\n",
    "    \"backbone_c2f1\": 2,\n",
    "    \"backbone_c2f2\": 4,\n",
    "    \"backbone_c2f3\": 6,\n",
    "    \"backbone_c2f4\": 8, \n",
    "    \"backbone_sppf\": 9,\n",
    "    \"neck_c2f1\": 12,\n",
    "    \"neck_c2f2\": 15,\n",
    "    \"neck_c2f3\": 18,\n",
    "    \"neck_c2f4\": 21\n",
    "}\n",
    "layers = {layer: yolo_nn[idx] for layer, idx in layer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size:torch.Size([1, 3, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 10499.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   -0.17793,    -0.54538,    -0.30789,     0.89351,     -1.0285,    -0.57948,     0.42836,     0.10964,    -0.22254,     0.30541,    -0.13108,     0.50289,     0.70809,     0.21787,      0.7684,     0.96136,     0.15447,   -0.031733,     0.23718,    -0.28627,     0.99356,   -0.060679,     0.97221,      0.7215,\n",
      "           0.42536,     0.01343,     0.70368,     0.74691,    -0.22701,     0.78145,    -0.34733,     0.66829], dtype=float32)]\n",
      "\n",
      "0: 640x640 2 class0s, 11 class1s, 17 class2s, 6 class3s, 23 class4s, 2 class5s, 11 class6s, 28 class7s, 23 class8s, 1 class9, 20 class10s, 1 class16, 6 class17s, 2 class18s, 2 class20s, 1 class24, 21 class25s, 5 class26s, 17 class34s, 19 class35s, 5 class36s, 26 class37s, 50 class38s, 1 class39, 11.2ms\n",
      "Speed: 0.0ms preprocess, 11.2ms inference, 24.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model class info:{0: 'class0', 1: 'class1', 2: 'class2', 3: 'class3', 4: 'class4', 5: 'class5', 6: 'class6', 7: 'class7', 8: 'class8', 9: 'class9', 10: 'class10', 11: 'class11', 12: 'class12', 13: 'class13', 14: 'class14', 15: 'class15', 16: 'class16', 17: 'class17', 18: 'class18', 19: 'class19', 20: 'class20', 21: 'class21', 22: 'class22', 23: 'class23', 24: 'class24', 25: 'class25', 26: 'class26', 27: 'class27', 28: 'class28', 29: 'class29', 30: 'class30', 31: 'class31', 32: 'class32', 33: 'class33', 34: 'class34', 35: 'class35', 36: 'class36', 37: 'class37', 38: 'class38', 39: 'class39', 40: 'class40', 41: 'class41', 42: 'class42'}\n",
      "Model summary: 129 layers, 3,019,233 parameters, 0 gradients, 8.2 GFLOPs\n",
      "tensor size:torch.Size([1, 3, 640, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 11999.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   -0.10052,    -0.39013,     0.97843,    -0.14276,   -0.091489,     0.14391,   -0.097336,      0.1942,    -0.29609,     0.19283,    -0.16842,    0.022666,    0.068624,    -0.26249,   -0.068183,    -0.19202,    0.057412,    -0.34517,    -0.55415,     0.20765,    -0.66342,    0.037181,      -0.602,     0.10561,\n",
      "          -0.50607,     0.14876,   -0.053002,    -0.20149,   -0.011167,    -0.10457,       0.109,    -0.64382], dtype=float32)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 18 class0s, 19 class1s, 26 class2s, 5 class3s, 49 class4s, 1 class5, 9 class6s, 27 class7s, 8 class8s, 2 class9s, 27 class10s, 1 class11, 1 class16, 2 class17s, 3 class20s, 12 class25s, 3 class26s, 1 class32, 13 class34s, 19 class35s, 2 class36s, 22 class37s, 27 class38s, 3 class40s, 6.7ms\n",
      "Speed: 0.0ms preprocess, 6.7ms inference, 13.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import yolov8_grad_cam as v8cam\n",
    "cam_params = {\n",
    "        'weight': '../models/yolov8-gtsrb.pt', # 现在只需要指定权重即可,不需要指定cfg\n",
    "        'device': 'cuda:0',\n",
    "        'method': 'SSGradCAM',\n",
    "        'layer': [2],\n",
    "        'backward_type': 'all', # detect:<class, box, all> segment:<class, box, segment, all> pose:<box, keypoint, all> obb:<box, angle, all> classify:<all>\n",
    "        'conf_threshold': 0, # 0.2\n",
    "        'ratio': 0.02, # 0.02-0.1\n",
    "        'show_result': True, # 不需要绘制结果请设置为False\n",
    "        'renormalize': False, # 需要把热力图限制在框内请设置为True(仅对detect,segment,pose有效)\n",
    "        'task':'detect', # 任务(detect,segment,pose,obb,classify)\n",
    "        'img_size':640, # 图像尺寸\n",
    "    }\n",
    "\n",
    "cam(r'../datasets/gtsrb_attack/0/images/00000_00000_00008.png', './result')\n",
    "cam = v8cam.yolo_heatmap(**cam_params)\n",
    "cam(r'../datasets/gtsrb_origin/0/images/00000_00000_00008.png', './result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c3e81",
   "metadata": {},
   "source": [
    "各c2f层CAM结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ece51d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_cam_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cam_backbone_c2f2 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\u001b[33m\"\u001b[39m\u001b[33mbackbone_c2f2\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      2\u001b[39m cam_backbone_c2f3 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\u001b[33m\"\u001b[39m\u001b[33mbackbone_c2f3\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m cam_backbone_c2f4 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\u001b[33m\"\u001b[39m\u001b[33mbackbone_c2f4\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'get_cam_result' is not defined"
     ]
    }
   ],
   "source": [
    "cam_backbone_c2f2 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f2\"])\n",
    "cam_backbone_c2f3 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f3\"])\n",
    "cam_backbone_c2f4 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_c2f4\"])\n",
    "cam_backbone_sppf = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"backbone_sppf\"])\n",
    "cam_neck_c2f1 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f1\"])\n",
    "cam_neck_c2f2 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f2\"])\n",
    "cam_neck_c2f3 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f3\"])\n",
    "cam_neck_c2f4 = get_cam_result(img=rgb_img, model=yolo_model, input_tensor=input_tensor, chosen_layers=[\"neck_c2f4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2957407",
   "metadata": {},
   "source": [
    "**评估指标:**\n",
    "\n",
    "**ROAD(Region Of Attention Discirmination):** (有问题:`ClassifierOutputTarget(category=17)`这一部分意义不明)\n",
    "\n",
    "- `ROADMostRelevantFirstAverage`: 逐步移除置信度从高到地的CAM区域,比较移除前后模型预测置信度的区别,相差越大,代表高相关区域定位准确.\n",
    "- `ROADLeastRelevantFirstAverage`: 逐步移除置信度从低到高的CAM区域,比较移除前后模型预测置信度的区别,相差越小,代表无关区域没有被错误定位到."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.metrics.road import ROADMostRelevantFirstAverage\n",
    "from pytorch_grad_cam.metrics.road import ROADLeastRelevantFirstAverage\n",
    "\n",
    "def eval_ROADMostRelevantFirst(model, input_tensor, grayscale_cams, target_classes) -> None:\n",
    "    cam_metric = ROADMostRelevantFirstAverage(percentiles=[20, 40, 60, 80])\n",
    "    # all_layers is a dict, but target_layers must be list\n",
    "\n",
    "    scores = cam_metric(\n",
    "        model=model, input_tensor=input_tensor, cams=grayscale_cams, targets=target_classes,\n",
    "    )\n",
    "    # print(scores.size)\n",
    "    score = scores.sum()\n",
    "    print(f\"The average confidence increase with ROAD accross 4 thresholds: {score}\")\n",
    "\n",
    "\n",
    "# You can also average across different percentiles, and combine\n",
    "# (LeastRelevantFirst - MostRelevantFirst) / 2\n",
    "\n",
    "batch_cam_backbone_c2f1 = torch.from_numpy(cam_backbone_c2f1).unsqueeze(dim=0)\n",
    "\n",
    "eval_ROADMostRelevantFirst(\n",
    "    model=yolo.model, input_tensor=input_tensor, grayscale_cams=batch_cam_backbone_c2f1.numpy(), target_classes=[ClassifierOutputTarget(category=17)]\n",
    ")\n",
    "\n",
    "def eval_ROADLeastRelevantFirstAverage(model, input_tensor, grayscale_cams, target_classes) -> None:\n",
    "    cam_metric = ROADLeastRelevantFirstAverage(percentiles=[20, 40, 60, 80])\n",
    "    # all_layers is a dict, but target_layers must be list\n",
    "\n",
    "    scores = cam_metric(\n",
    "        model=model,\n",
    "        input_tensor=input_tensor,\n",
    "        cams=grayscale_cams,\n",
    "        targets=target_classes,\n",
    "    )\n",
    "    # print(scores.size)\n",
    "    score = scores.sum()\n",
    "    print(f\"The average confidence increase with ROAD accross 4 thresholds: {score}\")\n",
    "\n",
    "\n",
    "# You can also average across different percentiles, and combine\n",
    "# (LeastRelevantFirst - MostRelevantFirst) / 2\n",
    "\n",
    "eval_ROADLeastRelevantFirstAverage(\n",
    "    model=yolo.model,\n",
    "    input_tensor=input_tensor,\n",
    "    grayscale_cams=batch_cam_backbone_c2f1.numpy(),\n",
    "    target_classes=[ClassifierOutputTarget(category=17)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875af718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=9, figsize=(8, 8))\n",
    "\n",
    "axes[0].imshow(cam_backbone_c2f1, cmap=\"gray\")\n",
    "axes[0].set_title(\"cam_b_c2f1\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(cam_backbone_c2f2, cmap=\"gray\")\n",
    "axes[1].set_title(\"cam_b_c2f2\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(cam_backbone_c2f3, cmap=\"gray\")\n",
    "axes[2].set_title(\"cam_b_c2f3\")\n",
    "axes[2].axis(\"off\")\n",
    "axes[3].imshow(cam_backbone_c2f4, cmap=\"gray\")\n",
    "axes[3].set_title(\"cam_b_c2f4\")\n",
    "axes[3].axis(\"off\")\n",
    "axes[4].imshow(cam_backbone_sppf, cmap=\"gray\")\n",
    "axes[4].set_title(\"cam_backbone_sppf\")\n",
    "axes[4].axis(\"off\")\n",
    "axes[5].imshow(cam_neck_c2f1, cmap=\"gray\")\n",
    "axes[5].set_title(\"cam_neck_c2f1\")\n",
    "axes[5].axis(\"off\")\n",
    "axes[6].imshow(cam_neck_c2f2, cmap=\"gray\")\n",
    "axes[6].set_title(\"cam_neck_c2f2\")\n",
    "axes[6].axis(\"off\")\n",
    "axes[7].imshow(cam_neck_c2f3, cmap=\"gray\")\n",
    "axes[7].set_title(\"cam_neck_c2f3\")\n",
    "axes[7].axis(\"off\")\n",
    "axes[8].imshow(cam_neck_c2f4, cmap=\"gray\")\n",
    "axes[8].set_title(\"cam_neck_c2f4\")\n",
    "axes[8].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d35086",
   "metadata": {},
   "source": [
    "**SSIM(Structural Similarity，结构相似性)**\n",
    "\n",
    "通过比较原始样本和对抗样本的热力图, 衡量注意力区域是否被显著改变, SSIM 越低 → 热力图结构差异越大（攻击更有效）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35abf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity  \n",
    "\n",
    "# im1, im2分别表示参与计算的图像数据\n",
    "# data_range表示图像数据的范围，一般设置为255或者1(如果对图像数据做了归一化操作，则为1)\n",
    "# channel_axis表示颜色通道位于图像的第几维度，如果不指定的话，则默认输入灰度图像\n",
    "ssim = structural_similarity(\n",
    "    cam_backbone_c2f1,\n",
    "    cam_backbone_c2f4,\n",
    "    win_size=None,\n",
    "    gradient=False,\n",
    "    data_range=1,\n",
    "    channel_axis=None,\n",
    "    multichannel=False,\n",
    "    gaussian_weights=False,\n",
    "    full=False,\n",
    ")\n",
    "\n",
    "print(f\"SSIM of cam_backbone_c2f1 & cam_backbone_c2f2: {ssim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a01b6",
   "metadata": {},
   "source": [
    "**在masked_pgd对抗攻击上测试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686045c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_traffic = YOLO(model='./models/yolov8-traffic.pt')\n",
    "yolo_traffic = yolo_traffic.cuda()\n",
    "yolo_traffic_model = yolo_traffic.model.to(device)\n",
    "yolo_traffic_nn = yolo_traffic_model.model\n",
    "layers = {layer: yolo_traffic_nn[idx] for layer, idx in layer_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_img = cv2.imread(filename=\"./images/origin_img_2.jpg\", flags=1)\n",
    "rgb_origin_img = origin_img.copy()[:, :, ::-1]\n",
    "rgb_origin_img = cv2.resize(src=rgb_origin_img, dsize=(224, 224))\n",
    "rgb_origin_img = np.float32(rgb_origin_img) / 255\n",
    "input_tensor_origin_img = preprocess_image(img=rgb_origin_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(dtype=torch.float32, device=device)  # type: ignore\n",
    "perturbed_img = cv2.imread(filename=\"./images/perturbed_img_2.jpg\", flags=1)\n",
    "rgb_perturbed_img = perturbed_img.copy()[:, :, ::-1]\n",
    "rgb_perturbed_img = cv2.resize(src=rgb_perturbed_img, dsize=(224, 224))\n",
    "rgb_perturbed_img = np.float32(rgb_perturbed_img) / 255\n",
    "input_tensor_perturbed_img = preprocess_image(img=rgb_perturbed_img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).to(dtype=torch.float32, device=device)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "cams_origin_img = {\n",
    "    layer: get_cam_result(\n",
    "        img=rgb_origin_img,\n",
    "        model=yolo_traffic_model,\n",
    "        input_tensor=input_tensor_origin_img,\n",
    "        chosen_layers=[layer],\n",
    "        virtualize=False,\n",
    "    )\n",
    "    for layer in layer_map.keys()\n",
    "}\n",
    "\n",
    "cams_perturbed_img = {\n",
    "    layer: get_cam_result(\n",
    "        img=rgb_perturbed_img,\n",
    "        model=yolo_traffic_model,\n",
    "        input_tensor=input_tensor_perturbed_img,\n",
    "        chosen_layers=[layer],\n",
    "        virtualize=False,\n",
    "    )\n",
    "    for layer in layer_map.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1302eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SSIM of cam_origin_img and cam_perturbed_img: \")\n",
    "for chosen_layer in layer_map.keys():\n",
    "    # chosen_layer = \"backbone_c2f4\"  # 在这里选择要比较的层次\n",
    "    cam_origin_img = cams_origin_img[chosen_layer]\n",
    "    cam_perturbed_img = cams_perturbed_img[chosen_layer]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(4, 4))\n",
    "    axes[0].imshow(cam_origin_img, cmap=\"gray\")\n",
    "    axes[0].set_title(f\"orig_{chosen_layer}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(cam_perturbed_img, cmap=\"gray\")\n",
    "    axes[1].set_title(f\"attk_{chosen_layer}\")\n",
    "    axes[1].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    ssim = structural_similarity(cam_origin_img, cam_perturbed_img, data_range=1)\n",
    "    print(f\"{chosen_layer}: {ssim}\")\n",
    "    \n",
    "# layer_map = {\n",
    "#     # 如果要选择其他c2f输出,只需要更改这个字典\n",
    "#     \"backbone_c2f1\": 2,\n",
    "#     \"backbone_c2f2\": 4,\n",
    "#     \"backbone_c2f3\": 6,\n",
    "#     \"backbone_c2f4\": 8, \n",
    "#     \"backbone_sppf\": 9,\n",
    "#     \"neck_c2f1\": 12,\n",
    "#     \"neck_c2f2\": 15,\n",
    "#     \"neck_c2f3\": 18,\n",
    "#     \"neck_c2f4\": 21\n",
    "# }\n",
    "\n",
    "# S\n",
    "# SSIM of cam_origin_img and cam_perturbed_img: \n",
    "# backbone_c2f1: 0.9997942613480257\n",
    "# backbone_c2f2: 0.9988800610291236\n",
    "# backbone_c2f3: 0.9993543305044151\n",
    "# backbone_c2f4: 0.9972734578256705\n",
    "# backbone_sppf: 0.9987707076693472\n",
    "# neck_c2f1: 0.9793938244919129\n",
    "# neck_c2f2: 0.9689466165305084\n",
    "# neck_c2f3: 0.9822124863635567\n",
    "# neck_c2f4: 0.986672852255135\n",
    "\n",
    "# L\n",
    "# SSIM of cam_origin_img and cam_perturbed_img: \n",
    "# backbone_c2f1: 0.9636343326863902\n",
    "# backbone_c2f2: 0.9781642958550689\n",
    "# backbone_c2f3: 0.9551331287842689\n",
    "# backbone_c2f4: 0.9047849218385144\n",
    "# backbone_sppf: 0.036879426969998984\n",
    "# neck_c2f1: 0.8844158753992417\n",
    "# neck_c2f2: 0.8037270538466107\n",
    "# neck_c2f3: 0.5478848147437752\n",
    "# neck_c2f4: 0.957862519667803"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748485f7",
   "metadata": {},
   "source": [
    "选择中等大小目标的任务上的neck_c2f3卷积层进行逐层细化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fdf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载YOLO模型（以v8为例）\n",
    "neck_c2f3_nn = yolo_traffic_nn[layer_map['neck_c2f3']]\n",
    "# print(neck_c2f3_nn)\n",
    "# 存储特征图的字典\n",
    "features = {}\n",
    "\n",
    "# 定义钩子函数：保存当前层输出\n",
    "def hook_fn(module, input, output) -> None:\n",
    "    features[module.__repr__()] = output  # 或自定义唯一标识\n",
    "\n",
    "# 遍历c2f中的所有子模块\n",
    "def register_hooks(module) -> None:\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Conv2d):  # 筛选卷积层\n",
    "            print(child.type)\n",
    "            child.register_forward_hook(hook=hook_fn)\n",
    "        else:\n",
    "            register_hooks(module=child)\n",
    "        \n",
    "register_hooks(module=neck_c2f3_nn)\n",
    "        \n",
    "# 输入测试数据触发前向传播\n",
    "output = yolo(source='./images/perturbed_img_2.jpg')\n",
    "# 提取特征图（features字典中保存了各卷积层的输出）\n",
    "print(features.keys())  # 查看捕获的卷积层特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
